{
  "metadata": {
    "kernelspec": {
      "display_name": "ml1-arm64",
      "language": "python",
      "name": "ml1-arm64"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVM and Ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Italian Olives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](images/Italy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I found this data set in the RGGobi book (http://www.ggobi.org/book/), from which the above diagram is taken. It has \"the percentage composition of fatty acids\n",
        "found in the lipid fraction of Italian olive oils', with oils from 3 regions of Italy: the North, the South, and Sardinia. The regions themselves are subdivided into areas as shown in the map above. The source for this data is:\n",
        "\n",
        ">Forina, M., Armanino, C., Lanteri, S. & Tiscornia, E. (1983), Classification of Olive Oils from their Fatty Acid Composition, in Martens, H. and\n",
        "Russwurm Jr., H., eds, Food Research and Data Analysis, Applied Science\n",
        "Publishers, London, pp. 189\u2013214."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data/olives-cleaned.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### exploring globally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "pd.value_counts(df.areastring, sort=False).plot(kind=\"bar\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "pd.value_counts(df.regionstring, sort=False).plot(kind=\"barh\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Figuring the South of Italy by Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "dfsouth=df[df.regionstring=='South']\n",
        "dfsouth.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "dfnosouth=df[df.regionstring!='South']\n",
        "dfnosouth.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM on training data for Regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Diagram from Jesse Johnson's excellent Shape of Data;\n",
        "http://shapeofdata.wordpress.com/2013/05/14/linear-separation-and-support-vector-machines/\n",
        "\n",
        "\n",
        "![max margin](images/svm.png) \n",
        "\n",
        "The idea is to draw a line in space between the classes. But not any line, but the line which gives the `maximum margin` rectangle between points of different classes.\n",
        "\n",
        "from http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html :\n",
        "![support vectors](images/img1260.png)\n",
        "\n",
        "The points right at the boundary are called the support vectors, which is where the name comes from.\n",
        "\n",
        "But what if the separability is not so simple, and there are points intruding?\n",
        "\n",
        "![intrusion](images/svm21.png)\n",
        "\n",
        "Then the idea is to minimize the distance of the \"crossed over\" points from the separating line. These crossed over points are costed using \"slack\" vectors. You dont want too many of these.\n",
        "\n",
        "You obtain the line by minimizing the [Hinge Loss](http://en.wikipedia.org/wiki/Hinge_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Stolen from Jake's notebooks, above: https://github.com/jakevdp/ESAC-stats-2014\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
        "\n",
        "def plot_svc_decision_function(clf, ax=None):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
        "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    P = np.zeros_like(X)\n",
        "    for i, xi in enumerate(x):\n",
        "        for j, yj in enumerate(y):\n",
        "            P[i, j] = clf.decision_function([[xi, yj]])\n",
        "    return ax.contour(X, Y, P, colors='k',\n",
        "                      levels=[-1, 0, 1], alpha=0.5,\n",
        "                      linestyles=['--', '-', '--'])\n",
        "\n",
        "def plot_svm(N):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X, y)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
        "    plt.xlim(-1, 4)\n",
        "    plt.ylim(-1, 6)\n",
        "    plot_svc_decision_function(clf, plt.gca())\n",
        "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=200, facecolors='none')\n",
        "    \n",
        "#interact(plot_svm, N=[10, 200]);\n",
        "interact(plot_svm, N=widgets.IntSlider(min=10, max=200, step=10, value=90));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the points that mainly matter are the ones which are near the support vector. If new such points come in, the prediction can change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a SVM\n",
        "\n",
        "We just do two acids to start and illustrate. We'll try and see if an Olive oil is from Sardinia or not!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dfnosouth[['linoleic', 'arachidic']]\n",
        "X = dfnosouth[['linolenic', 'arachidic']]\n",
        "\n",
        "\n",
        "y = (dfnosouth.regionstring.values=='Sardinia')*1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q1: Standardize the data set the correct way\n",
        "\n",
        "The correct way, unlike what we have done earlier is to standardize the training and test sets separately. Why is this?\n",
        "\n",
        "You are **not supposed to have seen the test set**. If you do, you are contaminating the training set and your results and error will have an optimistic bias on the training set.\n",
        "\n",
        "Indeed you will need to standardize the test set data to do your predictions by the mean and the standard deviation learned on the training set!\n",
        "\n",
        "Store the standardized data in `Xtrain, Xtest, ytrain, ytest`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xtrain.shape, Xtest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now fit the SVM with a \"linear\" kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, s=50, cmap='spring', alpha=0.3)\n",
        "plot_svc_decision_function(clf, plt.gca())\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=100, edgecolors='k', facecolors='none')\n",
        "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, s=50, marker=\"s\", cmap='spring', alpha=0.5);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, s=50, cmap='spring', alpha=0.3)\n",
        "plot_svc_decision_function(clf, plt.gca())\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=100, edgecolors='k', facecolors='none')\n",
        "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=clf.predict(Xtest), s=50, marker=\"s\", cmap='spring', alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is our accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Kernel Trick\n",
        "\n",
        "Often in SVMs one uses the [kernel trick](http://en.wikipedia.org/wiki/Kernel_method), which maps a lower dimension to a higher one to make things separable.\n",
        "\n",
        "See (from above mentioned book)\n",
        "\n",
        "![](images/img1331.png)\n",
        "\n",
        "So lets see what using a Radial Gaussian kernel look like?\n",
        "\n",
        "$$e^{-\\gamma d(x_1,x_2)^2}$$\n",
        "\n",
        "\n",
        "### Q2: Train a radial kernel\n",
        "\n",
        "Train a radial kernel by looking up the documentation. For simplicity we'll start by using the default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(Xtrain[:, 0], Xtrain[:, 1], c=ytrain, s=50, cmap='spring', alpha=0.3)\n",
        "plot_svc_decision_function(clf, plt.gca())\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "                s=100, facecolors='none', edgecolors='k')\n",
        "plt.scatter(Xtest[:, 0], Xtest[:, 1], c=ytest, s=50, marker=\"s\", cmap='spring', alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Compute the confusion matric and the accuracy score!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf.score(Xtest, ytest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3: Use cross validation\n",
        "\n",
        "This is a take away for home. Use cross validation to estimate the hyper-parameters of this model. These are the regularization and the kernel radius.\n",
        "\n",
        "This page will help: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
        "\n",
        "Discuss on discourse!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detective work\n",
        "\n",
        "See if you can separate out sicilian olive oils using either SVM or logistic regression or kNN.\n",
        "\n",
        "When you do this you might learn something about how the olive oil industry works...\n",
        "\n",
        "Discuss on discourse!"
      ]
    }
  ]
}